data:
  dataset_path: null
  resize: null
  antialias: true
  scale: null
  clip_outliers: false
  to_tensor: false
  normalise: null

  sampling:
    in_size: [64, 64]
    in_channels: 3

  condition:
    mode: null
    resize: null
    antialias: false
    scale: null
    clip_outliers: false
    to_tensor: false
    normalise: false

logging:
  tool: tensorboard

autoencoder:
  model:
    in_channels: 3
    out_channels: 3
    feature_channels: 64
    z_channels: 4
    embbeded_channels: 4
    channels_mult: [1, 2, 4]
    num_res_blocks: 2
    recon_loss: l2
    div_loss: kl
    perceptual_loss: lpips
    adversarial_loss: true
    checkpoint: null
    condition:
      in_channels: 1
      feature_channels: 16

  params:
    div_weight: 0.001
    perceptual_weight: 1.
    recon_weight: 1.
    adversarial_weight: 1.
    wiener_epsilon: 250.
    wiener_alpha: -0.01
    wiener_beta: 3.5
    wiener_filter_scale: 2
    lpips_model: alex
    disc_n_layers: 3
    disc_feature_channels: 64
    adversarial_mode: "vanilla"
    adversarial_lr_factor: 1.

  training:
    batch_size: 32
    n_epochs: 100
    save_recon_freq: 100
    ckpt_freq: 100
    ckpt_last_only: true
    num_workers: 0
    sampling_freq: 0

  validation:
    split: 0.1
    batch_size: 32
    freq: 10
    save_recon_freq: 100
    num_workers: 0

  sampling:
    batch_size: 32
    n_samples: 64
    num_workers: 0
    enable: True

  optim:
    weight_decay: 0.000
    bias_weight_decay: false
    optimiser: "Adam"
    lr: 0.00002
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: null
    lr_scheduler:
      scheduler: null

diffusion:
  model:
    feature_channels: 128
    channels_mult: [1, 1, 2, 2]
    num_res_blocks: 2
    attention: [0, 1, 2, 3]
    num_transformer_layers: 1
    num_attn_heads: 8
    loss: l2
    checkpoint: null
    condition:
      in_channels: 1
      feature_channels: 16

  params:
    beta_schedule: linear
    beta_start: 0.0001
    beta_end: 0.02
    num_diffusion_timesteps: 100
    latent_scaling_factor: 1.
    sampler: ddpm

  training:
    batch_size: 32
    n_epochs: 100
    sampling_freq: 50
    sampling_temperature: 0.5
    sampling_skip_steps: 0
    ckpt_freq: 100
    ckpt_last_only: true
    num_workers: 0

  validation:
    split: 0.1
    batch_size: 32
    freq: 10
    sampling_freq: 50
    num_workers: 0

  sampling:
    batch_size: 32
    temperature: 1.
    skip_steps: 0
    n_samples: 64
    num_workers: 0
    output: last_only
    enable: True

  optim:
    weight_decay: 0.000
    bias_weight_decay: false
    optimiser: "Adam"
    lr: 0.00002
    beta1: 0.9
    amsgrad: false
    eps: 0.00000001
    grad_clip: null

    lr_scheduler:
      scheduler: null
